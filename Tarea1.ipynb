{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf044b72-0aa3-4eb8-ae50-fc9215cd9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3bd9ab-2089-488c-93c6-1ae4fad15e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SalePrice' 'OverallQual' '1stFlrSF' 'TotRmsAbvGrd' 'YearBuilt'\n",
      "  'LotFrontage']\n",
      " ['208500.0' '7.0' '856.0' '8.0' '2003.0' '65.0']\n",
      " ['181500.0' '6.0' '1262.0' '6.0' '1976.0' '80.0']\n",
      " ...\n",
      " ['266500.0' '7.0' '1188.0' '9.0' '1941.0' '66.0']\n",
      " ['142125.0' '5.0' '1078.0' '5.0' '1950.0' '68.0']\n",
      " ['147500.0' '5.0' '1256.0' '6.0' '1965.0' '75.0']]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(\"proyecto_training_data.npy\")\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1), :] #Dataset contiene nans, los ignoro en la implementacion\n",
    "fields = [\"SalePrice\", \"OverallQual\", \"1stFlrSF\", \"TotRmsAbvGrd\", \"YearBuilt\", \"LotFrontage\"]\n",
    "print(np.vstack((np.array(fields), dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a6645cc-cb4a-43c9-bb83-c71f8f66f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    def __init__(self, batch_size = 16, lr=0.01, epochs = 1000):\n",
    "        self.logdir = f'logs\\\\lm_bs={batch_size}_lr={lr}_epochs={epochs}'\n",
    "        self.writer = tf.summary.create_file_writer(self.logdir)\n",
    "        self.m = None\n",
    "        self.b = None\n",
    "        \n",
    "        self.batch_size = batch_size \n",
    "        self.lr = lr \n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def h(self, m, b, x):\n",
    "        with tf.name_scope(\"casting_variables\"):\n",
    "            y = tf.cast(m, tf.float64)*tf.cast(x, tf.float64) + tf.cast(b, tf.float64)\n",
    "        return y\n",
    "    \n",
    "    def get_params(self):\n",
    "        return((self.m, self.b))\n",
    "    \n",
    "    def error(self, y,y_pred):\n",
    "        with tf.name_scope(\"error_calculation\"):\n",
    "            return 1/2*tf.reduce_mean(tf.math.square(y - y_pred), name=\"MSE_Calc\")\n",
    "\n",
    "    def __call__(self,x):\n",
    "        return self.h(self.m, self.b, x)\n",
    "    \n",
    "    @tf.function\n",
    "    def actual_training(self, x, y):\n",
    "        with tf.name_scope(\"main_graph\"):\n",
    "            batch_size = self.batch_size\n",
    "            lr = self.lr \n",
    "            epochs = self.epochs\n",
    "            error = None\n",
    "            with tf.name_scope(\"var_creation\"):\n",
    "                if self.m is None:\n",
    "                    self.m = tf.Variable(initial_value=0.0, name=\"slope\")\n",
    "                if self.b is None:\n",
    "                    self.b = tf.Variable(initial_value=0.0, name=\"intercept\")\n",
    "\n",
    "            iterations = int(len(y)/batch_size)\n",
    "            step = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(iterations): \n",
    "                    batch_start = i*batch_size\n",
    "                    batch_end = batch_start + batch_size\n",
    "                    #x_mb = tf.reshape(x[batch_start:batch_end], [-1,1])\n",
    "                    #y_mb = tf.reshape(y[batch_start:batch_end], [-1,1])\n",
    "                    x_mb = x[batch_start:batch_end]\n",
    "                    y_mb = y[batch_start:batch_end]\n",
    "\n",
    "                    with tf.name_scope(\"gradient_calc\"):\n",
    "                        with tf.GradientTape() as grad_tape:\n",
    "                            grad_tape.watch(self.b)\n",
    "                            grad_tape.watch(self.m)\n",
    "\n",
    "                            y_pred = self.h(self.m, self.b, x_mb)\n",
    "\n",
    "                            error = self.error(y_mb, y_pred)\n",
    "\n",
    "                            # calcular el gradiente de la funcion de costo respecto de los parametros\n",
    "                            grad_m,grad_b = grad_tape.gradient(error,[self.m, self.b])\n",
    "\n",
    "                    with tf.name_scope(\"parameter_updating\"):\n",
    "                        # actualizar los parametros dando un paso en direccion contraria al gradiente\n",
    "                        self.m.assign(self.m - lr*grad_m)\n",
    "                        self.b.assign(self.b - lr*grad_b)\n",
    "\n",
    "                    tf.summary.scalar('MSE', error, step=step)\n",
    "                    step += 1\n",
    "            final_params = (self.m, self.b)\n",
    "        return final_params\n",
    "        \n",
    "    \n",
    "    def train(self, x, y):\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.trace_on(graph=True, profiler=False)\n",
    "            (final_params) = self.actual_training(x, y)\n",
    "            tf.summary.trace_export(\n",
    "                                  name=\"model_graph\",\n",
    "                                  step=0,\n",
    "                                  profiler_outdir=\"logs\\\\modelgraph\")\n",
    "        self.writer.flush()\n",
    "\n",
    "        return final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f2bef79-3af8-469d-8da2-e2861a044fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=120.16186>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=18.077047>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(False)\n",
    "model1 = LinearModel(epochs = 1, lr = 0.0001, batch_size=len(dataset[:,0]))\n",
    "model1.train(dataset[:,1], dataset[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af9e964-a4dd-4a0d-b980-fddf6c1a6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(138.23890495300293, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(model1(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a14e79da-4029-4dae-892e-3c008e46ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'main_graph/var_creation/slope:0' shape=() dtype=float32, numpy=120.16186>,\n",
       " <tf.Variable 'main_graph/var_creation/intercept:0' shape=() dtype=float32, numpy=18.077047>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a73a35-51e9-48ee-abd2-3e98566c13e7",
   "metadata": {},
   "source": [
    "# Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e3f00-bf64-41f5-a8e3-c430b08f865e",
   "metadata": {},
   "source": [
    "Hipotesis: Un LR alto causara una reduccion de error mas rapidamente pero causara convergencia mas tardia\n",
    "Un epoch alto alcanzara convergencia de error antes de que termine de entrenar todas la epochs\n",
    "Un batch_size bajo podra suplir la misma informacion que un batch size alto, pero se tardara mas en entrenarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1001ac7e-a7e5-44f5-9868-b000c72e211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model with params epochs: 100, lr: 0.1, batch_size: 400\n",
      "WARNING:tensorflow:Trace already enabled\n",
      "Done\n",
      "\n",
      "training model with params epochs: 100, lr: 0.1, batch_size: 600\n",
      "Done\n",
      "\n",
      "training model with params epochs: 1000, lr: 0.1, batch_size: 400\n",
      "Done\n",
      "\n",
      "training model with params epochs: 1000, lr: 0.1, batch_size: 600\n",
      "Done\n",
      "\n",
      "training model with params epochs: 100, lr: 0.01, batch_size: 400\n",
      "Done\n",
      "\n",
      "training model with params epochs: 100, lr: 0.01, batch_size: 600\n",
      "Done\n",
      "\n",
      "training model with params epochs: 1000, lr: 0.01, batch_size: 400\n",
      "Done\n",
      "\n",
      "training model with params epochs: 1000, lr: 0.01, batch_size: 600\n",
      "Done\n",
      "\n",
      "training model with params epochs: 100, lr: 0.001, batch_size: 400\n",
      "Done\n",
      "\n",
      "training model with params epochs: 100, lr: 0.001, batch_size: 600\n",
      "Done\n",
      "\n",
      "training model with params epochs: 1000, lr: 0.001, batch_size: 400\n",
      "Done\n",
      "\n",
      "training model with params epochs: 1000, lr: 0.001, batch_size: 600\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "tf.config.run_functions_eagerly(True)\n",
    "lrs = [0.1, 0.01, 0.001]\n",
    "epochs = [100, 1000]\n",
    "batch_sizes = [400, 600]\n",
    "\n",
    "for lr, epoch, batch_size in list(itertools.product(lrs, epochs, batch_sizes)):\n",
    "    print(f\"training model with params epochs: {epoch}, lr: {lr}, batch_size: {batch_size}\")\n",
    "    model = LinearModel(epochs = epoch, lr = lr, batch_size = batch_size)\n",
    "    model.train(dataset[:,1], dataset[:,0])\n",
    "    print(\"Done\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3add60-ddf9-4fbd-bd5f-ae483ac8bb99",
   "metadata": {},
   "source": [
    "# Graficas de TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75370370-1eda-437f-8307-7a83b7005bb8",
   "metadata": {},
   "source": [
    "![alt text](ALL_MSE.PNG \"MSEs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7e093-e656-4450-b383-d855c151e068",
   "metadata": {},
   "source": [
    "Parece que existen basicamente dos tipos de modelo, aquellos que superaron alguna especie de barrera que les permitio reducir mucho mas el error y aquellos que no lo lograron.\n",
    "\n",
    "Vemos que con los modelos con mas iteraciones totales si lograron converger a un error mas bajo en general, asi que veamos estos modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb106479-2fc7-45c6-9952-18d09c5af844",
   "metadata": {},
   "source": [
    "![alt text](Interesting_models.PNG \"Modelos interesantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17b133-2287-43ec-8ff3-77659522422f",
   "metadata": {},
   "source": [
    "Parece ser que la \"barrera\" a superar es el learning rate, siendo un lr de 0.01 capaz de reducir de mejor manera el error del modelo. Veamos estos modelos mas a fondo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826dbdb4-51f1-46ea-bb12-f0c3c238f039",
   "metadata": {},
   "source": [
    "![alt text](best_models.PNG \"Mejores modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c536f44-d04e-406e-8804-ab6ae50f3d6d",
   "metadata": {},
   "source": [
    "Podemos ver que un batch size mas pequeño tiene el potencial de obtener un error mas pequeño, pero la varianza entre cada iteracion es bastante grande y levanta sospechas sobre su confiabilidad\n",
    "\n",
    "Por otra parte, un batch size mas grande no solo logra reducir la variabilidad, sino que tambien parece converger en menos iteraciones, por lo que resulta un modelo mas rapido de entrenar\n",
    "\n",
    "Podemos concluir entonces que el mejor modelo tiene bastantes epochs (1000), con un batch size mas grande (600) y un lr intermedio (0.01) de entre los hiperparametros entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88994a56-730a-41ea-a5e3-9ce76a0994e1",
   "metadata": {},
   "source": [
    "### Grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ac50f-ee96-411e-9552-fe82d72ee3ce",
   "metadata": {},
   "source": [
    "![alt text](graph.PNG \"Grafo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
